import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
from datetime import datetime
import sys
import os
from urllib.parse import urljoin, urlparse
import re
import concurrent.futures
from threading import Lock
import threading
import asyncio
import aiohttp
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# å…¨åŸŸè®Šæ•¸å’Œé–
results_lock = Lock()
print_lock = Lock()

class OptimizedScraper:
    def __init__(self, max_workers=20, batch_size=50):
        self.max_workers = max_workers
        self.batch_size = batch_size
        self.session = self.create_optimized_session()
        self.results = []
        self.processed_count = 0
        self.total_downloads = 0
        
        # é ç·¨è­¯æ­£å‰‡è¡¨é”å¼
        self.title_pattern = re.compile(r'å·¥æ¥­åŠæœå‹™æ¥­å—åƒ±å“¡å·¥å…¨å¹´ç¸½è–ªè³‡ä¸­ä½æ•¸åŠåˆ†å¸ƒçµ±è¨ˆçµæœ')
        self.year_pattern = re.compile(r'(\d{2,3})å¹´')
        self.filename_pattern = re.compile(r'[<>:"/\\|?*\s]+')
        
    def create_optimized_session(self):
        """å‰µå»ºå„ªåŒ–çš„requests session"""
        session = requests.Session()
        
        # è¨­å®šé‡è©¦ç­–ç•¥
        retry_strategy = Retry(
            total=2,
            backoff_factor=0.1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=50,
            pool_maxsize=50,
            pool_block=False
        )
        
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # å„ªåŒ–çš„headers
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        return session

    def print_progress(self, current, total, found_count, current_title=""):
        """é¡¯ç¤ºé€²åº¦æ¢å’Œç•¶å‰ç‹€æ…‹ï¼ˆç·šç¨‹å®‰å…¨ï¼‰"""
        with print_lock:
            progress = (current) / total * 100
            bar_length = 30
            filled_length = int(bar_length * progress / 100)
            bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
            
            sys.stdout.write('\r')
            sys.stdout.write(f'é€²åº¦: [{bar}] {progress:.1f}% ({current}/{total}) | å·²æ‰¾åˆ°: {found_count} ç­†')
            if current_title:
                sys.stdout.write(f' | ç•¶å‰: {current_title[:50]}...')
            sys.stdout.flush()

    def is_target_title(self, title_text):
        """åŒ¹é…æ‰€æœ‰å¹´ä»½çš„è–ªè³‡çµ±è¨ˆçµæœï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
        if not title_text:
            return False
        
        # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦é€²è¡Œæ¯”è¼ƒ
        cleaned_title = ''.join(title_text.split())
        return bool(self.title_pattern.search(cleaned_title))

    def extract_year_from_title(self, title_text):
        """å¾æ¨™é¡Œä¸­æå–å¹´ä»½ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
        match = self.year_pattern.search(title_text)
        return match.group(1) if match else "æœªçŸ¥å¹´ä»½"

    def get_download_links_fast(self, soup, base_url):
        """å¿«é€Ÿç‰ˆæœ¬çš„ä¸‹è¼‰é€£çµæå–"""
        download_links = []
        
        # åªæœç´¢æœ€é‡è¦çš„é€£çµé¡å‹
        selectors = [
            'a[href*="Upload"]',
            'a[href$=".pdf"]',
            'a[href$=".xlsx"]',
            'a[href$=".xls"]',
        ]
        
        seen_urls = set()
        
        for selector in selectors:
            try:
                links = soup.select(selector)
                for link in links[:5]:  # é™åˆ¶æ¯ç¨®é¡å‹æœ€å¤š5å€‹
                    href = link.get('href')
                    if href and href not in seen_urls:
                        full_url = urljoin(base_url, href)
                        if full_url not in seen_urls:
                            seen_urls.add(full_url)
                            download_links.append({
                                'url': full_url,
                                'text': link.get_text(strip=True) or 'ä¸‹è¼‰é€£çµ',
                                'type': self.get_file_type(full_url)
                            })
            except Exception:
                continue
                
        return download_links

    def get_file_type(self, url):
        """å¾URLåˆ¤æ–·æª”æ¡ˆé¡å‹ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
        url_lower = url.lower()
        if '.pdf' in url_lower:
            return 'PDF'
        elif '.xlsx' in url_lower or '.xls' in url_lower:
            return 'Excel'
        elif '.doc' in url_lower:
            return 'Word'
        elif '.zip' in url_lower:
            return 'ZIP'
        else:
            return 'å…¶ä»–'

    def process_batch(self, sid_batch, base_url, download_folder):
        """æ‰¹æ¬¡è™•ç†å¤šå€‹é é¢"""
        batch_results = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_sid = {
                executor.submit(self.process_single_page_fast, sid, base_url, download_folder): sid 
                for sid in sid_batch
            }
            
            for future in concurrent.futures.as_completed(future_to_sid):
                sid = future_to_sid[future]
                try:
                    success, result = future.result()
                    if success:
                        batch_results.append(result)
                except Exception as e:
                    pass
                
                self.processed_count += 1
                
                # æ¯è™•ç†10å€‹é¡¯ç¤ºé€²åº¦
                if self.processed_count % 10 == 0:
                    self.print_progress(self.processed_count, self.total_ids, len(self.results))
        
        return batch_results

    def process_single_page_fast(self, sid, base_url, download_folder):
        """å¿«é€Ÿè™•ç†å–®å€‹é é¢"""
        url = base_url + str(sid)
        
        try:
            # æ›´çŸ­çš„è¶…æ™‚æ™‚é–“
            resp = self.session.get(url, timeout=5)
            
            if resp.status_code != 200:
                return False, None
            
            # ä½¿ç”¨lxmlè§£æå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰æ›´å¿«
            try:
                soup = BeautifulSoup(resp.text, "lxml")
            except:
                soup = BeautifulSoup(resp.text, "html.parser")
            
            # å¿«é€Ÿæ¨™é¡Œæœå°‹
            title = self.find_title_fast(soup)
            
            if not title:
                return False, None
                
            title_text = title.get_text(strip=True)
            
            if self.is_target_title(title_text):
                year = self.extract_year_from_title(title_text)
                
                # å¿«é€Ÿç²å–ä¸‹è¼‰é€£çµ
                download_links = self.get_download_links_fast(soup, url)
                
                result = {
                    "sid": sid, 
                    "title": title_text, 
                    "year": year,
                    "url": url,
                    "download_links": download_links,
                    "download_count": len(download_links),
                    "found_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                # ç·šç¨‹å®‰å…¨åœ°æ‰“å°çµæœ
                with print_lock:
                    print(f"\nâœ… æ‰¾åˆ°ï¼šID {sid} ({year}å¹´) - {len(download_links)} å€‹æª”æ¡ˆ")
                
                # éåŒæ­¥ä¸‹è¼‰æª”æ¡ˆï¼ˆå¯é¸ï¼‰
                if download_links:
                    self.download_files_batch(download_links, year, download_folder)
                
                return True, result
                
        except Exception as e:
            return False, None
        
        return False, None

    def find_title_fast(self, soup):
        """å¿«é€Ÿå°‹æ‰¾æ¨™é¡Œ"""
        # ç›´æ¥æœç´¢åŒ…å«é—œéµå­—çš„æ–‡æœ¬
        target_text = soup.find(string=lambda text: text and 'å·¥æ¥­åŠæœå‹™æ¥­å—åƒ±å“¡å·¥å…¨å¹´ç¸½è–ªè³‡ä¸­ä½æ•¸åŠåˆ†å¸ƒçµ±è¨ˆçµæœ' in text)
        if target_text:
            return target_text.parent
        
        # å‚™ç”¨æœç´¢
        for tag in ['h1', 'h2', 'h3']:
            title = soup.find(tag)
            if title:
                return title
        
        return None

    def download_files_batch(self, download_links, year, download_folder):
        """æ‰¹æ¬¡ä¸‹è¼‰æª”æ¡ˆ"""
        year_folder = os.path.join(download_folder, f"{year}å¹´")
        os.makedirs(year_folder, exist_ok=True)
        
        # ä½¿ç”¨è¼ƒå°‘çš„ç·šç¨‹æ•¸é¿å…è¢«ä¼ºæœå™¨å°é–
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            for i, link in enumerate(download_links[:3]):  # é™åˆ¶æ¯å€‹é é¢æœ€å¤šä¸‹è¼‰3å€‹æª”æ¡ˆ
                file_extension = os.path.splitext(urlparse(link['url']).path)[1] or '.pdf'
                safe_filename = self.create_safe_filename(link['text'], year)
                filename = os.path.join(year_folder, f"{safe_filename}_{i+1}{file_extension}")
                
                future = executor.submit(self.download_file, link['url'], filename)
                futures.append((future, filename, link['text']))
            
            for future, filename, link_text in futures:
                try:
                    success, error = future.result(timeout=10)
                    if success:
                        self.total_downloads += 1
                except:
                    pass

    def create_safe_filename(self, text, year=""):
        """å‰µå»ºå®‰å…¨çš„æª”åï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
        safe_chars = self.filename_pattern.sub('_', text)
        safe_chars = safe_chars[:50]  # é™åˆ¶é•·åº¦
        
        if year:
            return f"{year}å¹´_{safe_chars}"
        return safe_chars

    def download_file(self, url, filename, max_retries=1):
        """ä¸‹è¼‰æª”æ¡ˆï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
        try:
            response = self.session.get(url, stream=True, timeout=10)
            response.raise_for_status()
            
            with open(filename, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return True, None
        except Exception as e:
            return False, str(e)

    def scrape_salary_news(self, start_id=230000, end_id=235000):
        """ä¸»è¦çˆ¬å–å‡½æ•¸ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
        base_url = "https://www.dgbas.gov.tw/News_Content.aspx?n=3602&s="
        
        self.results = []
        self.processed_count = 0
        self.total_downloads = 0
        self.total_ids = end_id - start_id
        
        # å‰µå»ºä¸‹è¼‰è³‡æ–™å¤¾
        download_folder = f"è–ªè³‡çµ±è¨ˆä¸‹è¼‰_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(download_folder, exist_ok=True)
        
        print(f"ğŸš€ é–‹å§‹é«˜é€Ÿçˆ¬å–è–ªè³‡æ–°èç¨¿...")
        print(f"ç¯„åœ: {start_id} - {end_id} (å…± {self.total_ids} å€‹ID)")
        print(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}, æœ€å¤§ç·šç¨‹æ•¸: {self.max_workers}")
        print(f"ä¸‹è¼‰è³‡æ–™å¤¾: {download_folder}")
        print(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("-" * 80)
        
        start_time = time.time()
        
        # åˆ†æ‰¹è™•ç†
        all_ids = list(range(start_id, end_id))
        
        for i in range(0, len(all_ids), self.batch_size):
            batch = all_ids[i:i + self.batch_size]
            batch_results = self.process_batch(batch, base_url, download_folder)
            
            with results_lock:
                self.results.extend(batch_results)
            
            # æ¯æ‰¹æ¬¡å¾Œé¡¯ç¤ºç‹€æ…‹
            if i % (self.batch_size * 4) == 0:
                elapsed = time.time() - start_time
                speed = self.processed_count / elapsed if elapsed > 0 else 0
                with print_lock:
                    print(f"\nğŸ“Š å·²è™•ç† {self.processed_count}/{self.total_ids} | é€Ÿåº¦: {speed:.1f} é /ç§’ | æ‰¾åˆ°: {len(self.results)} ç­†")
        
        elapsed_time = time.time() - start_time
        
        print("\n" + "=" * 80)
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ç¸½å…±è™•ç†: {self.processed_count} å€‹ID")
        print(f"æ‰¾åˆ°ç¬¦åˆæ¢ä»¶: {len(self.results)} ç­†")
        print(f"ç¸½ä¸‹è¼‰æª”æ¡ˆ: {self.total_downloads} å€‹")
        print(f"ç¸½è€—æ™‚: {elapsed_time:.2f} ç§’")
        print(f"å¹³å‡é€Ÿåº¦: {self.processed_count/elapsed_time:.2f} é /ç§’")
        print(f"ä¸‹è¼‰è³‡æ–™å¤¾: {download_folder}")
        
        return self.results, download_folder

def save_results(results, download_folder):
    """å„²å­˜çµæœåˆ°CSV"""
    if not results:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è³‡æ–™")
        return
    
    # æº–å‚™CSVè³‡æ–™
    csv_data = []
    for result in results:
        base_info = {
            'sid': result['sid'],
            'year': result['year'],
            'title': result['title'],
            'url': result['url'],
            'download_count': result['download_count'],
            'found_time': result['found_time']
        }
        
        if result['download_links']:
            for i, link in enumerate(result['download_links']):
                row = base_info.copy()
                row.update({
                    'download_url': link['url'],
                    'download_text': link['text'],
                    'file_type': link['type']
                })
                csv_data.append(row)
        else:
            csv_data.append(base_info)
    
    df = pd.DataFrame(csv_data)
    filename = os.path.join(download_folder, "è–ªè³‡çµ±è¨ˆæ¸…å–®.csv")
    df.to_csv(filename, index=False, encoding="utf-8-sig")
    
    print(f"âœ… å·²å„²å­˜æ¸…å–®åˆ°: {filename}")
    print("\nğŸ“‹ æ‰¾åˆ°çš„è³‡æ–™:")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['year']}å¹´ - {result['download_count']} å€‹æª”æ¡ˆ")

if __name__ == "__main__":
    try:
        # å‰µå»ºå„ªåŒ–çš„çˆ¬èŸ²å¯¦ä¾‹
        scraper = OptimizedScraper(max_workers=20, batch_size=100)
        
        # é–‹å§‹çˆ¬å–
        results, download_folder = scraper.scrape_salary_news(start_id=230000, end_id=235000)
        
        # å„²å­˜çµæœ
        save_results(results, download_folder)
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç¨‹å¼è¢«ä½¿ç”¨è€…ä¸­æ–·")
    except Exception as e:
        print(f"\n\nâŒ ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()